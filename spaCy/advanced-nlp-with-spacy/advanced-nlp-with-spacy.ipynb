{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nlp object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f7cecf49f28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = English()\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Hello world!')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## token object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "world"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[1]\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## span object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "world!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[1:4]\n",
    "span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  [0, 1, 2, 3, 4]\n",
      "Text:  ['It', 'costs', '$', '5', '.']\n",
      "is_alpha:  [True, True, False, False, False]\n",
      "is_punct:  [False, False, False, False, True]\n",
      "like_num:  [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('It costs $5.')\n",
    "print('Index: ', [token.i for token in doc])\n",
    "print('Text: ', [token.text for token in doc])\n",
    "print('is_alpha: ', [token.is_alpha for token in doc])\n",
    "print('is_punct: ', [token.is_punct for token in doc])\n",
    "print('like_num: ', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Statistical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('She ate the pizza')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_dep(doc):\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "syntactic_dep(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entities(doc):\n",
    "    for token in doc.ents:\n",
    "        print(token.text, token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "named_entities(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ORG\n",
      "Definition: Companies, agencies, institutions, etc.\n",
      "\n",
      "Label: GPE\n",
      "Definition: Countries, cities, states\n",
      "\n",
      "Label: MONEY\n",
      "Definition: Monetary values, including unit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc.ents:\n",
    "    print('Label: {0}\\nDefinition: {1}\\n'.format(token.label_, spacy.explain(token.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Rule-based matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.matcher.matcher.Matcher at 0x7f7cecf375e8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ORTH': 'iPhone'}, {'ORTH': 'X'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = [{'ORTH':'iPhone'}, {'ORTH':'X'}]\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.matcher.matcher.Matcher at 0x7f7cecf375e8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9528407286733565721, 1, 3)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('New iPhone X release date leaked')\n",
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_pattern(doc, pattern):\n",
    "    import spacy\n",
    "    from spacy.matcher import Matcher\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add('TEMP_PATTERN', None, pattern)\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        matched_span = doc[start:end]\n",
    "        print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {'IS_DIGIT':True},\n",
    "    {'LOWER':'fifa'},\n",
    "    {'LOWER':'world'},\n",
    "    {'LOWER':'cup'},\n",
    "    {'IS_PUNCT':True},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('2018 FIFA World Cup: France won!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "matching_pattern(doc, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {'LEMMA':'love', 'POS':'VERB'},\n",
    "    {'POS':'NOUN'},\n",
    "]\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "matching_pattern(doc, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator & quantifiers\n",
    "\n",
    "```python\n",
    "{'OP': '!'} # negation: match 0 times\n",
    "{'OP': '?'} # optional: 0 / 1 time\n",
    "{'OP': '+'} # >= 1 time(s)\n",
    "{'OP': '*'} # >= 0 time(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {'LEMMA':'buy'},\n",
    "    {'POS':'DET', 'OP':'?'},\n",
    "    {'POS':'NOUN'},\n",
    "]\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "matching_pattern(doc, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON nsubj bought\n",
      "bought VERB ROOT bought\n",
      "a DET det smartphone\n",
      "smartphone NOUN dobj bought\n",
      ". PUNCT punct bought\n",
      "Now ADV advmod buying\n",
      "I PRON nsubj buying\n",
      "'m VERB aux buying\n",
      "buying VERB ROOT buying\n",
      "apps NOUN dobj buying\n",
      ". PUNCT punct buying\n"
     ]
    }
   ],
   "source": [
    "syntactic_dep(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data structures: Vocab, Lexemes & StringStore\n",
    "\n",
    "## Shared Vocab & string store\n",
    "Vocab: stores data shared across multiple documents\n",
    "\n",
    "Strings are only stored onced in `StringStore` via `nlp.vocab.strings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3197928453018144401"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee_hash = nlp.vocab.strings['coffee']\n",
    "coffee_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These 2 will return KeyError'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''These 2 will return KeyError'''\n",
    "# coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "# nlp.vocab.strings[3197928453018144401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coffee hash value: 3197928453018144401\n",
      "String value: coffee\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print('Coffee hash value:', nlp.vocab.strings['coffee'])\n",
    "print('String value:', nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexemes: entries in the vocabulary\n",
    "\n",
    "- Context-independent info\n",
    "    - Text `lexeme.text` and hash `lexeme.orth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: coffee; Hash: 3197928453018144401; Alphabet: True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "print('Text: {0}; Hash: {1}; Alphabet: {2}'.format(lexeme.text, lexeme.orth, lexeme.is_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. Data Structures: Doc, Span & Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello world!"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello world"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = Span(doc, 0, 2)\n",
    "span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello world"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_with_label = Span(doc, 0, 2, label='GREETING')\n",
    "span_with_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Hello world,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents = [span_with_label]\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Word vectors & semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627204117787385\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare 2 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73695457\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I like pizza and pasta\")\n",
    "\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare doc with span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32531983166759537\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare span with doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6199092090831612\n"
     ]
    }
   ],
   "source": [
    "span = nlp(\"I like pizza & pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vector\n",
    "\n",
    "spaCy predict similarity using word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n",
      " -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n",
      " -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n",
      "  5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n",
      "  1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n",
      " -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n",
      "  1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n",
      " -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n",
      "  2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n",
      "  6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n",
      " -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n",
      " -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n",
      "  1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n",
      "  5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n",
      " -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n",
      "  4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n",
      " -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n",
      " -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n",
      "  4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n",
      "  1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n",
      "  5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n",
      "  5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n",
      "  8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n",
      " -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n",
      " -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n",
      "  3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n",
      "  4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n",
      "  4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n",
      "  2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n",
      " -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n",
      "  2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n",
      " -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n",
      "  2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n",
      " -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n",
      "  3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n",
      "  3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n",
      " -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n",
      "  8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n",
      "  2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n",
      " -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n",
      " -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n",
      "  2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n",
      " -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n",
      "  1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n",
      " -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n",
      " -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n",
      "  5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n",
      "  4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n",
      "  1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n",
      " -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('I have a banana')\n",
    "\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9501446702124066"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = nlp('I like cats')\n",
    "doc2 = nlp('I hate cats')\n",
    "\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Combining models & rules\n",
    "\n",
    "1. Rule-based systems\n",
    "2. Statistical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_pattern_md(doc, pattern, method):\n",
    "    import spacy\n",
    "    from spacy.matcher import Matcher, PhraseMatcher\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    \n",
    "    if method == 'Matcher':\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "    elif method == 'PhraseMatcher':\n",
    "        matcher = PhraseMatcher(nlp.vocab)\n",
    "    else:\n",
    "        raise ValueError(\"Matcher method must be Matcher / PhraseMatcher\")\n",
    "    \n",
    "    matcher.add('TEMP_PATTERN', None, pattern)\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        print('Matched span:', span.text)\n",
    "        print('Root token:', span.root.text)\n",
    "        print('Root head token:', span.root.head.text)\n",
    "        print('Previous token:', doc[start-1].text, doc[start-1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I have a Golden Retriever"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('I have a Golden Retriever')\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lower': 'golden'}, {'lower': 'retriever'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = [{'lower':'golden'}, {'lower':'retriever'}]\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n",
      "CPU times: user 13.6 s, sys: 444 ms, total: 14.1 s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "matching_pattern_md(doc, pattern, 'Matcher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Golden retriever"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = nlp('Golden retriever')\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 s, sys: 392 ms, total: 14.3 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "matching_pattern_md(doc, pattern, 'PhraseMatcher')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f7cecbd6128>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f7cecb45a08>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f7cecb45a68>)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Custom pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f7cecc1cda0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_component(doc):\n",
    "    print('Doc length:', len(doc))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe(custom_component, first=True)\n",
    "\n",
    "print('Pipeline:', nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Hello world!"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('Hello world!')\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. Extension attributes\n",
    "\n",
    "1. Attribute extensions\n",
    "2. Property extensions\n",
    "3. Method extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "Doc.set_extension('title', default=None)\n",
    "Token.set_extension('is_color', default=False)\n",
    "Span.set_extension('has_color', default=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 5\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('The Sky is blue.')\n",
    "doc[3]._.is_color=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_is_color(token):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return token.text in colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension('is_color', getter=get_is_color, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue - True\n"
     ]
    }
   ],
   "source": [
    "print(doc[3].text, '-', doc[3]._.is_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_is_color(span):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return any(token.text in colors for token in span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Span.set_extension('has_color', getter=get_is_color, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 5\n",
      "True - sky is blue\n",
      "True - is blue.\n",
      "False - The sky\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('The sky is blue.')\n",
    "\n",
    "print(doc[1:4]._.has_color, '-', doc[1:4].text)\n",
    "print(doc[2:5]._.has_color, '-', doc[2:5].text)\n",
    "print(doc[0:2]._.has_color, '-', doc[0:2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc.set_extension('has_token', method=has_token, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n",
      "False - cloud\n"
     ]
    }
   ],
   "source": [
    "print(doc._.has_token('blue'), '- blue')\n",
    "print(doc._.has_token('cloud'), '- cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3. Scaling and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This is a text', {'id': 1, 'page_number': 15}),\n",
       " ('Add another text', {'id': 2, 'page_number': 16})]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    ('This is a text', {'id':1, 'page_number':15}),\n",
    "    ('Add another text', {'id':2, 'page_number':16}),\n",
    "]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 4\n",
      "Doc length: 3\n",
      "This is a text 15\n",
      "Add another text 16\n"
     ]
    }
   ],
   "source": [
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context['page_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc.set_extension('id', default=None)\n",
    "Doc.set_extension('page_number', default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 4\n",
      "Doc length: 3\n",
      "This is a text\n",
      "Add another text\n"
     ]
    }
   ],
   "source": [
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context['id']\n",
    "    doc._.page_number = context['page_number']\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using only tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello world!"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.make_doc('Hello world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disable pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 3\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    doc = nlp('Hello world!')\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['custom_component', 'tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training and updating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    ('iPhone X is coming', {'entities': [(0,8,'GADGET')]}),\n",
    "    ('I need a new phone! Any tips?', {'entities': []}),\n",
    "    ('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET')]}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Blank English model'''\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "'''Create blank entity recognizer and add it to the pipeline'''\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "'''Add a new label'''\n",
    "ner.add_label('GADGET')\n",
    "\n",
    "nlp.begin_training()\n",
    "\n",
    "for i in range(10):\n",
    "    '''Shuffle the training data'''\n",
    "    random.shuffle(training_data)\n",
    "    \n",
    "    '''Create batches and iterate over them'''\n",
    "    for batch in spacy.util.minibatch(training_data):\n",
    "        \n",
    "        '''Split the batch in texts and annotations'''\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        \n",
    "        '''Update the model'''\n",
    "        nlp.update(texts, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('./C4-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
