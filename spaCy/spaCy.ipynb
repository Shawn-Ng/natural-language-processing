{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install models and process text\n",
    "```python\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp_de = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Here', 'are', 'two', 'sentences', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Hello, world. Here are two sentences.')\n",
    "\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ich', 'bin', 'ein', 'Berliner', '.']\n"
     ]
    }
   ],
   "source": [
    "doc_de = nlp_de(u'Ich bin ein Berliner.')\n",
    "\n",
    "print([t.text for t in doc_de])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tokens, noun chunks & sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peach\n",
      "emoji\n",
      "üçë\n",
      "outranking eggplant\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Peach emoji is where it has always been. Peach is the superior \"\n",
    "          u\"emoji. It's outranking eggplant üçë \")\n",
    "\n",
    "print(doc[0].text)          # Peach\n",
    "print(doc[1].text)          # emoji\n",
    "print(doc[-1].text)         # üçë\n",
    "print(doc[17:19].text)      # outranking eggplant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peach emoji\n",
      "[Peach emoji, it, Peach, the superior emoji, It, eggplant üçë]\n"
     ]
    }
   ],
   "source": [
    "noun_chunks = list(doc.noun_chunks)\n",
    "\n",
    "print(noun_chunks[0].text)  # Peach emoji\n",
    "print(noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Peach emoji is where it has always been., Peach is the superior emoji., It's outranking eggplant üçë]\n"
     ]
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "\n",
    "assert len(sentences) == 3\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-grained POS tag PROPN 95\n",
      "Coarse-grained POS tag NNP 15794550382381185553\n",
      "Word shape Xxxxx 16072095006890171862\n",
      "Alphanumeric characters? True\n",
      "Punctuation mark? False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "apple = doc[0]\n",
    "\n",
    "print('Fine-grained POS tag', apple.pos_, apple.pos)\n",
    "print('Coarse-grained POS tag', apple.tag_, apple.tag)\n",
    "print('Word shape', apple.shape_, apple.shape)\n",
    "print('Alphanumeric characters?', apple.is_alpha)\n",
    "print('Punctuation mark?', apple.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit? False\n",
      "Like a number? True\n",
      "Like an email address? False\n"
     ]
    }
   ],
   "source": [
    "billion = doc[10]\n",
    "\n",
    "print('Digit?', billion.is_digit)\n",
    "print('Like a number?', billion.like_num)\n",
    "print('Like an email address?', billion.like_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use hash values for any string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401 coffee\n",
      "3197928453018144401 3197928453018144401\n",
      "coffee coffee\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I love coffee')\n",
    "\n",
    "coffee_hash = nlp.vocab.strings[u'coffee']  # 3197928453018144401\n",
    "coffee_text = nlp.vocab.strings[coffee_hash]  # 'coffee'\n",
    "\n",
    "print(coffee_hash, coffee_text)\n",
    "print(doc[2].orth, coffee_hash)  # 3197928453018144401\n",
    "print(doc[2].text, coffee_text)  # 'coffee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3073001599257881079 beer\n"
     ]
    }
   ],
   "source": [
    "beer_hash = doc.vocab.strings.add(u'beer')  # 3073001599257881079\n",
    "beer_text = doc.vocab.strings[beer_hash]  # 'beer'\n",
    "\n",
    "print(beer_hash, beer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17758882941175878347 ü¶Ñ \n"
     ]
    }
   ],
   "source": [
    "unicorn_hash = doc.vocab.strings.add(u'ü¶Ñ ')  # 18234233413267120783\n",
    "unicorn_text = doc.vocab.strings[unicorn_hash]  # 'ü¶Ñ '\n",
    "\n",
    "print(unicorn_hash, unicorn_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco 0 13 GPE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'San Francisco considers banning sidewalk delivery robots')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB 0 2 ORG\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "doc = nlp(u'FB is hiring a new VP of global policy')\n",
    "doc.ents = [Span(doc, 0, 1, label=doc.vocab.strings[u'ORG'])]\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and update neural network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import random\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "train_data = [(\"Uber blew through $1 million\", {'entities': [(0, 4, 'ORG')]})]\n",
    "\n",
    "with nlp.disable_pipes(*[pipe for pipe in nlp.pipe_names if pipe != 'ner']):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(10):\n",
    "        random.shuffle(train_data)\n",
    "        for text, annotations in train_data:\n",
    "            nlp.update([text], [annotations], sgd=optimizer)\n",
    "\n",
    "nlp.to_disk('/model')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize a dependency parse and named entities in your browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Serving on port 5000...\u001b[0m\n",
      "    Using the 'dep' visualizer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [12/Jun/2018 19:39:45] \"GET / HTTP/1.1\" 200 3057\n",
      "127.0.0.1 - - [12/Jun/2018 19:39:45] \"GET /favicon.ico HTTP/1.1\" 200 3057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Shutting down server on port 5000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc_dep = nlp(u'This is a sentence.')\n",
    "displacy.serve(doc_dep, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Serving on port 5000...\u001b[0m\n",
      "    Using the 'ent' visualizer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [12/Jun/2018 19:40:10] \"GET / HTTP/1.1\" 200 1691\n",
      "127.0.0.1 - - [12/Jun/2018 19:40:10] \"GET /favicon.ico HTTP/1.1\" 200 1691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Shutting down server on port 5000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_ent = nlp(u'When Sebastian Thrun started working on self-driving cars at Google '\n",
    "              u'in 2007, few people outside of the company took him seriously.')\n",
    "displacy.serve(doc_ent, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get word vectors and similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "!python -m spacy download en_core_web_md\n",
    "!python -m spacy download en_vectors_web_lg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple <-> banana 0.5831844\n",
      "pasta <-> hippo 0.07934912\n",
      "True True True True\n"
     ]
    }
   ],
   "source": [
    "# nlp = spacy.load('en_core_web_md')\n",
    "# vectors_web_lg gives best result\n",
    "nlp = spacy.load('en_vectors_web_lg')\n",
    "doc = nlp(u\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
    "\n",
    "apple = doc[0]\n",
    "banana = doc[2]\n",
    "pasta = doc[6]\n",
    "hippo = doc[8]\n",
    "\n",
    "print('apple <-> banana', apple.similarity(banana))\n",
    "print('pasta <-> hippo', pasta.similarity(hippo))\n",
    "print(apple.has_vector, banana.has_vector, pasta.has_vector, hippo.has_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple and efficient serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/explosion/spacy-notebooks/blob/master/notebooks/lightning_tour.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
